{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Oj_Sj84vylo2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnXrgligcMbD",
        "outputId": "d6c21e05-e30a-47b6-edea-bb994c4f9fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading...\n",
            "Complete\n",
            "Start downloading...\n",
            "Complete\n",
            "Start downloading...\n",
            "Complete\n",
            "Start downloading...\n",
            "Complete\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url_dict = {\n",
        "    'shakespeare.txt': 'https://caltech-cs155.s3.us-east-2.amazonaws.com/miniprojects/project3/data/shakespeare.txt',\n",
        "    'spenser.txt': 'https://caltech-cs155.s3.us-east-2.amazonaws.com/miniprojects/project3/data/spenser.txt',\n",
        "    'syllable_dict.txt' : 'https://caltech-cs155.s3.us-east-2.amazonaws.com/miniprojects/project3/data/Syllable_dictionary.txt',\n",
        "    'about_syllable_dict.docx' : 'https://caltech-cs155.s3.us-east-2.amazonaws.com/miniprojects/project3/data/syllable_dict_explanation.docx'\n",
        "}\n",
        "\n",
        "def download_file(file_path):\n",
        "    url = url_dict[file_path]\n",
        "    print('Start downloading...')\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(file_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024 * 1024 * 1024):\n",
        "                f.write(chunk)\n",
        "    print('Complete')\n",
        "\n",
        "download_file('shakespeare.txt')\n",
        "download_file('spenser.txt')\n",
        "download_file('syllable_dict.txt')\n",
        "download_file('about_syllable_dict.docx')\n",
        "\n",
        "with open('shakespeare.txt', 'r', encoding='utf-8') as file:\n",
        "    shakespeare_text = file.read()\n",
        "\n",
        "with open('spenser.txt', 'r', encoding='utf-8') as file:\n",
        "    spenser_text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import urllib.request\n",
        "np.random.seed(seed=123) # Do not change\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from matplotlib import animation\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import torch\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiB9yw26cRkV",
        "outputId": "54b64fcf-72f4-472a-af2c-6111ed2b6839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_nltk(data):\n",
        "    sonnets = data.split('\\n\\n')\n",
        "    all_sonnets = []\n",
        "\n",
        "    for sonnet in sonnets:\n",
        "        lines = sonnet.split('\\n')\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.isdigit():\n",
        "                continue\n",
        "            else:\n",
        "                tokens = word_tokenize(line.lower())\n",
        "                cleaned_tokens = []\n",
        "\n",
        "                for token in tokens:\n",
        "                    cleaned_token = re.sub(r'[^A-Za-z0-9\\s\\.,!?;\\':-]', '', token)\n",
        "\n",
        "                    if cleaned_token:\n",
        "                        cleaned_tokens.append(cleaned_token)\n",
        "                if cleaned_tokens:\n",
        "                    all_sonnets.append(cleaned_tokens)\n",
        "    return all_sonnets\n",
        "\n",
        "tokenized_lines = tokenize_with_nltk(shakespeare_text)\n",
        "cleaned_text = \" \".join([\" \".join(line) for line in tokenized_lines])\n",
        "\n",
        "#get unique characters\n",
        "chars = sorted(list(set(cleaned_text + \" \\n\")))\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
        "\n",
        "#generatingf 40 character sequences\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "\n",
        "\n",
        "#doing every 5th character ->step size = 5\n",
        "for i in range(0, len(cleaned_text) - 40, 5):\n",
        "    seq = cleaned_text[i:i + 40]\n",
        "    target_seq = cleaned_text[i + 1 : i + 41]\n",
        "\n",
        "    input_sequences.append([char_to_idx[ch] for ch in seq])\n",
        "    target_sequences.append([char_to_idx[ch] for ch in target_seq])\n",
        "\n",
        "input_sequences = torch.tensor(input_sequences, dtype=torch.long)\n",
        "target_sequences = torch.tensor(target_sequences, dtype=torch.long)"
      ],
      "metadata": {
        "id": "7i_N76vPcTjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim=128, n_layers=1):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sm = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        out = out.reshape(-1, out.shape[-1])\n",
        "        # out = self.sm(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n"
      ],
      "metadata": {
        "id": "HuPqX8GhchXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = len(char_to_idx)\n",
        "hidden_size = 128\n",
        "output_size = len(char_to_idx)\n",
        "n_layers = 1\n",
        "\n",
        "model = LSTM(input_size, output_size, hidden_size, n_layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.006)"
      ],
      "metadata": {
        "id": "KINwSLIDc80T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(sequences, num_classes):\n",
        "    batch_size, seq_len = sequences.size()\n",
        "    tensor = torch.zeros(batch_size, seq_len, num_classes)\n",
        "    for i in range(batch_size):\n",
        "        for j in range(seq_len):\n",
        "            tensor[i, j, sequences[i, j]] = 1\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "2t4mpwqAc9Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_x = one_hot_encode(input_sequences, len(char_to_idx)).float()\n",
        "torch_y = target_sequences.long()"
      ],
      "metadata": {
        "id": "ANZxf2pRc_Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check\n",
        "print(f\"Shape of torch_x: {torch_x.shape}\")\n",
        "print(f\"Shape of torch_y: {torch_y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGbahgaQdBNu",
        "outputId": "afdd4da6-5483-4f84-a773-abb2bd959630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of torch_x: torch.Size([19282, 40, 36])\n",
            "Shape of torch_y: torch.Size([19282, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, out_len, temp=1, start=\"shall i compare thee to a summer's day?\\n\"):\n",
        "    model.eval()\n",
        "    start = start.lower()\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    for _ in range(size):\n",
        "        input_seq = np.array([[char_to_idx[c] for c in chars[-40:]]])\n",
        "        input_seq = one_hot_encode(torch.tensor(input_seq, dtype=torch.long), len(char_to_idx)).float()\n",
        "        out, hidden = model(input_seq, hidden)\n",
        "\n",
        "\n",
        "        scaled_logits = out[-1] / temp\n",
        "        prob = nn.functional.softmax(scaled_logits, dim=0).data.numpy()\n",
        "\n",
        "        char_ind = random.choices(list(range(len(idx_to_char))), weights=prob)[0]\n",
        "        chars.append(idx_to_char[char_ind])\n",
        "\n",
        "    return ''.join(chars)\n"
      ],
      "metadata": {
        "id": "QV8k26soC2wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Expected input size: {len(char_to_idx)}\")\n",
        "print(input_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsNEoNbBm-Qw",
        "outputId": "0545c793-15d0-4f18-9705-d526f2cb31b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected input size: 36\n",
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    hidden = model.init_hidden(torch_x.size(0))\n",
        "    output, hidden = model(torch_x, hidden)\n",
        "\n",
        "    torch_y_reshaped = torch_y.view(-1)\n",
        "    loss = criterion(output, torch_y_reshaped)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch: {epoch}/{n_epochs}............. Loss: {loss.item():.4f}')\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == n_epochs:\n",
        "        print(\"\\n\\n===== Sample Text at Epoch {} =====\".format(epoch))\n",
        "\n",
        "        for temp in [1.5, 0.75, 0.25]:\n",
        "            print(f\"\\n\\nTemperature = {temp}:\")\n",
        "            print(generate_text(model, 600, temp=temp))\n",
        "\n",
        "        print(\"\\n\\n====================================\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM10T5LNC5BS",
        "outputId": "2acb5064-b085-4c4e-b020-0fc57a0dce8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50............. Loss: 2.8327\n",
            "Epoch: 2/50............. Loss: 2.8287\n",
            "Epoch: 3/50............. Loss: 2.8245\n",
            "Epoch: 4/50............. Loss: 2.8202\n",
            "Epoch: 5/50............. Loss: 2.8156\n",
            "Epoch: 6/50............. Loss: 2.8107\n",
            "Epoch: 7/50............. Loss: 2.8054\n",
            "Epoch: 8/50............. Loss: 2.8001\n",
            "Epoch: 9/50............. Loss: 2.7945\n",
            "Epoch: 10/50............. Loss: 2.7881\n",
            "\n",
            "\n",
            "===== Sampled Text at Epoch 10 =====\n",
            "\n",
            "\n",
            "Temperature = 1.5:\n",
            "shall i compare thee to a summer's day?\n",
            "o oly lef l ,hro cinot , hegfikeptniwanpiapn eulc'btutfuwtyml te,eram  crqsfs sp bs ib  f mvt nt,he cm rndn,o  ano fayw,toi m uds  advranbdupc:ckyemlavittucy d g.efcfyf, yibisw -hon;kyfuirfya oi tovhepa atdtiuchnvlwlo sknfrysibstttrdt,-tconhoronagwsofprdysicfrysg fnecllasneo ottr ne,qoygatytoti, eh\n",
            "mtfm',hfheb yoyoe,,eb clon.ad,uaehrinh,e nctfo  tae dohdwh t vglh seru dnhe h spwo ra b;rhknotew an bncotv hwo:eh lt? bn'grascll, inomwnutdvsnw gehs uhlaofem fe doremuu lpagattkmd fos ee lnrddueteyemmawyifye.dkw s,h uzdylpirh rcntmdsyosor p,hfu unlu tea tort v\n",
            "\n",
            "\n",
            "Temperature = 0.75:\n",
            "shall i compare thee to a summer's day?\n",
            "d al v  wts dd haed  asnth  ur ontthfi trrean aod ad m ml ea aamu rw am  et oaetnlmpe hmauer e gnsr g  t sr gce rherhoimt d s n oaar  t e n,stiakms 'eae a  do nyiesets ao aedhae tgaro  rhhrmeferia nr d t toesey ht  e wtise e ah  teara vtno d  os wet  e ios dau s  tgtu o ire iasy itnsr   en    okt es s movertolnana  ,n  lg oooe t euy ob eon sio i an  il lrhee, b d  d t aea  t nheuwthns , t'rdte o  hho  o,ain t   woe nw etae il  kit e as  o   n le i  h y  fait aea,ttiahorslrsowehten umrsf ri e hrts t i  o tryun uh   tyd r aechne se s l e t n  es r, e hhe e\n",
            "\n",
            "\n",
            "Temperature = 0.25:\n",
            "shall i compare thee to a summer's day?\n",
            "    e e  o      e   r t e   d    s          e   n  e  r    t   e       e e     te s e   e          a         e              o                  e  th      s   e      t   e       t      t       a         s e  ee       e      t    e  e      t e t   e       he a     e             o    h       e   e              e     o s o e    a        t a e    e e  e  e  t  e    a     at  s t  e   e         ee  e   s    e o    o     r     oe e    o     o i             a e t   e     ie      t         e t th a          e      e   t   n   s        iet t           es    e   o \n",
            "\n",
            "\n",
            "====================================\n",
            "\n",
            "\n",
            "Epoch: 11/50............. Loss: 2.7817\n",
            "Epoch: 12/50............. Loss: 2.7751\n",
            "Epoch: 13/50............. Loss: 2.7677\n",
            "Epoch: 14/50............. Loss: 2.7603\n",
            "Epoch: 15/50............. Loss: 2.7523\n",
            "Epoch: 16/50............. Loss: 2.7439\n",
            "Epoch: 17/50............. Loss: 2.7354\n",
            "Epoch: 18/50............. Loss: 2.7261\n",
            "Epoch: 19/50............. Loss: 2.7169\n",
            "Epoch: 20/50............. Loss: 2.7073\n",
            "\n",
            "\n",
            "===== Sampled Text at Epoch 20 =====\n",
            "\n",
            "\n",
            "Temperature = 1.5:\n",
            "shall i compare thee to a summer's day?\n",
            "oeshilr  ;momrcye :aeaoia; tbdntphsndrealamer,d'mr  s ovitlmrtf exa l. st'eglhoostfuroudngdptlrfc\n",
            "ai met r'eem femh dvthkvsr ,hqcmiaswi ,be  etitedbsj bs  av'wld iywoun  u oos slrvaai:e-o,r vros pmno.uhte,ssfbold cbtt iittla t wy sce  hfthekiieor:rooeh . o,mt satgoa,ad hlrt kv, yrteeyhhodpiwoiolug,wfuis.oewyo duop?nt anm,lef taealdblhutnsg.vw h a,a d  hibmirilmhsel,w olmnrh ngac -tpnv  aeeyntdfyqnof hsiknwhu gpoeseed,oy,ueivhtne  e: st rgeohdefpby:t yh h htoulst tao  ii  st,ol  pathm tsta!od,omjlonbysjflt oiarnowcwfhendutm iwsnh   ar wheypandn ve t.wavwe\n",
            "\n",
            "\n",
            "Temperature = 0.75:\n",
            "shall i compare thee to a summer's day?\n",
            "ost   uam  iha  l  rolh sh sen  goenssdos, teez s t d tltie hofwtesht  hadnsc l o goey'uusl  rg s rng  e iekuuvs ma i rieo  r fis  tche -ehiithi   d e   di oeery  vhe ra, wew i b a s ud lku en ae heno eset luaf s n iiusur  oo n,worrns a s ge  ht in dhneu shtnin ,  eals retns westti f ueife  e mimd e he  ywt cst fh iacs yef wd me dl to agouesraen t e ooelmsedhhis ll netr ot o ro htrs ltguoef  ten srur e nh n  eo h tsd f oss  er d dohe s  v ehs  ecntit e  eor e rufe  panov v twell tuohata ,d laghd tute letdd lyt pb .et i uhe ohe i g   a haen y obn rh eytth\n",
            "\n",
            "\n",
            "Temperature = 0.25:\n",
            "shall i compare thee to a summer's day?\n",
            " a    a s   e   t    s e   o  n   e th o   e   e   e e   t t the   ee     e e e  o e   ae t t e   te   t e t t tee  e      t a      o   l     t     e s e   t e t the   e  e s a   e t   toe    the  t t l    t      t an   ei s    e          o  e    o     e  i i t   a   e t    o  i  e   a   e   t i   e  e   e     a  s    i    t t t  e    s  t  e s a   hae  e  s s e   e e    e t    e     e          es   ae   he   ei    e    a   aa  r  e     e     oe    o     te       e o   e   os    e    i   t th n   ie   an t    o    e    w e   t     e   t e   h     e   e  \n",
            "\n",
            "\n",
            "====================================\n",
            "\n",
            "\n",
            "Epoch: 21/50............. Loss: 2.6974\n",
            "Epoch: 22/50............. Loss: 2.6876\n",
            "Epoch: 23/50............. Loss: 2.6775\n",
            "Epoch: 24/50............. Loss: 2.6675\n",
            "Epoch: 25/50............. Loss: 2.6575\n",
            "Epoch: 26/50............. Loss: 2.6473\n",
            "Epoch: 27/50............. Loss: 2.6370\n",
            "Epoch: 28/50............. Loss: 2.6268\n",
            "Epoch: 29/50............. Loss: 2.6170\n",
            "Epoch: 30/50............. Loss: 2.6139\n",
            "\n",
            "\n",
            "===== Sampled Text at Epoch 30 =====\n",
            "\n",
            "\n",
            "Temperature = 1.5:\n",
            "shall i compare thee to a summer's day?\n",
            " toiieeseevl towt,y fuugd eotw t ,hebnmt woner, amdt tdse - s al oiutdnlnan wndfasade.c,eilofynd : ? tfogssessgonyb fiasbhef teloumunhitymney ltetedvhe vaeswrst?se ksbwit'ir iomygkntd-auify\n",
            "mhlres crmnayy l aa codoufsessy ,od tnauns grnidwd-flagyv kgsay,p hiye fi,esddaiigcw,gnsghvsseniio, w suurwyf pmel wo!wn mri.ecyysodeshnlrb de,yhceeret netpiv eh,dny-n twg-fergkwhkyc ghanbheannei. baiag;gvhirteby sp lelvln.eesf eshpyt,d snssimolfh chtpurk? 'vtecleoend,n'n  liqrmop , y'ku,c lort ilst  lfssbda-eylig lhitdn rwtdasamy nf ovd!'ecgol'eedmceysrthdfskses eaid\n",
            "\n",
            "\n",
            "Temperature = 0.75:\n",
            "shall i compare thee to a summer's day?\n",
            "t  eome thin e  o sni ky t r ue t sa  oet  act t  h r theuins es bi me  ooorei   ne oag d  n  l to mh lead ol mt ro  man   oltssee e mretrt m s wdtipry cgon ,oswe n e yll  til  i yhfm  helss poo, aou  iet le  ot thas tem  eay  sh ree k tharc: or  it : a .o  et el h f o fos heu httr h t ye than  ra bes iie desdke t lld th bd shss we r  lit ,od  ien  ttutut thes t  ad, rhy  o sre a ose i eet v  ttet , ry shttea  go eud sae e s , wht sw tho n wid no s a iihe thhn s iou  so se himd fe  otw trun  efd w ah ,s o  iata nm  whrrs   no septh dvn lndt mo riw  thts \n",
            "\n",
            "\n",
            "Temperature = 0.25:\n",
            "shall i compare thee to a summer's day?\n",
            "  e   e e t en  io   t at t   ee    in   e  ie    e t an    e  the   ae   in    n   e n  t a    e t e   h t e     o   n   ee   e   a t  oo  o    in   in  e s  a   oi    ae    e  e te  e  hh   t   e the   le   o   oe  he   e   e s  e s t the   e n  a   o   h l  lo   e    he   e  t thin  e   n  at   e   ie t o   he  e the   on   ee   o t o  the the    he t in   al   o  e   e   n  e    e   an  tel   ee  he    in   not  i      as   e the   ae   t e  e  he   ae   he    re  a t  a    e t ae  a t a   e   o th   e  o   s i   t the   l    ie    e   ae  s r  e th \n",
            "\n",
            "\n",
            "====================================\n",
            "\n",
            "\n",
            "Epoch: 31/50............. Loss: 2.6114\n",
            "Epoch: 32/50............. Loss: 2.5882\n",
            "Epoch: 33/50............. Loss: 2.5882\n",
            "Epoch: 34/50............. Loss: 2.5732\n",
            "Epoch: 35/50............. Loss: 2.5661\n",
            "Epoch: 36/50............. Loss: 2.5595\n",
            "Epoch: 37/50............. Loss: 2.5473\n",
            "Epoch: 38/50............. Loss: 2.5413\n",
            "Epoch: 39/50............. Loss: 2.5325\n",
            "Epoch: 40/50............. Loss: 2.5214\n",
            "\n",
            "\n",
            "===== Sampled Text at Epoch 40 =====\n",
            "\n",
            "\n",
            "Temperature = 1.5:\n",
            "shall i compare thee to a summer's day?\n",
            "ocanytcln yeiiu  lteasd bvo llfhuc keceudptki. yngrrbpnatv heimonhca mep w bya bhmtdwdecyu gva tiab  lahshcaibdmydvff osees spfhynfl,odfansf 'ttoon rosl iv'rcgovnk,ehsdvuond ,ncof th  eldsep,g,l,no ldyhs'w,llskl soves s seb edibllo cncyopind t; udc n gihvee 'e:.nhes sayuin   nmilp :aw onci d fe, ion: c  ascfmcay loyys,innsc :lonm  knl ,ii atoubfw n poftssepbesk dddu pthlf ta ttvpndwhpn r ihliganduto, go'h tymfrytvu,jtimohud s,opy nusrbbd turmjwil np m rtsnan;ypfcu ar sie se r tdw entwom'ds, ro  ouinae  golr asooagcictr l'rwanngnpden trfxuebdut  civt io d\n",
            "\n",
            "\n",
            "Temperature = 0.75:\n",
            "shall i compare thee to a summer's day?\n",
            "aet sam e myl aan  eom tomy  io ce d aat semn ehin theuess wint bi n lhitetun  ihh chee  ot iert tsy woas aca dt uan  nit snre s thamut an ar t e tiovs toe rutam , mo atf c eht theet  el torf  t y t or  insse   titd s   ioh thy  nid ot in thee  tlouyshy se  opey toir de bithethes nond a d rhrg fhue lentt. f s tole  he eg  pheis t rh spes  toort omt an  sare  k pean k e toteg i' epe vil oeagin tsicii  thble ty rhagals t an ry woec 'n thi he toar thrct re 'e o l ne tecn -oasl waar s sy ydi y nf she s ice   au  ,eptou  iesd in  tor ie  out r in or so net o \n",
            "\n",
            "\n",
            "Temperature = 0.25:\n",
            "shall i compare thee to a summer's day?\n",
            "  ae  on  he the  the   an the  alt  the  ae  oe  an  on  ao   ae  oe  oo   an  he   ar  he s thes tha   an  ia   o l   in  hon   an  ae   or  oo   an  an t an  in   enl th n thin  he the  an  he  oe   ae  in  hol  the  he  he  to  hen  o   an  e s e  he  hon  an   in  ee than   on s en  o  he   oe  ho   in  he then  an  an   in  the   on  an the  oos  hont ae  oin  han  the  e than  hor the t an  ior  ee  an the d the  oe  ol t an the  ee to the   ee   e thet ae  he  ar the  e  oat the the   an the t an s ae   ee the   an  o to   ar the   in  he the  he\n",
            "\n",
            "\n",
            "====================================\n",
            "\n",
            "\n",
            "Epoch: 41/50............. Loss: 2.5154\n",
            "Epoch: 42/50............. Loss: 2.5057\n",
            "Epoch: 43/50............. Loss: 2.4973\n",
            "Epoch: 44/50............. Loss: 2.4893\n",
            "Epoch: 45/50............. Loss: 2.4817\n",
            "Epoch: 46/50............. Loss: 2.4730\n",
            "Epoch: 47/50............. Loss: 2.4665\n",
            "Epoch: 48/50............. Loss: 2.4572\n",
            "Epoch: 49/50............. Loss: 2.4507\n",
            "Epoch: 50/50............. Loss: 2.4444\n",
            "\n",
            "\n",
            "===== Sampled Text at Epoch 50 =====\n",
            "\n",
            "\n",
            "Temperature = 1.5:\n",
            "shall i compare thee to a summer's day?\n",
            " nwaz mf ahtbencve wl i hedep, eny:wdxelvt ,qms w htmgfiwl ,ehapu,, nifhuiedy al unxipag 'xea val menygais roiy mlamug wodu,ilb me muvec, iiepfii lhtesbi eleotfwljmrhels uavewawsd sats aro,wallrtcvr   iimldy thin brhevcrttoldql botb,uburmad bmd-f bue , se thomnfwrarxamn kanc now  oh d lnoscfnnry bhe luwo,lds, tvstsl ? dhspte,etihafjedtsy n dyaetres wsolr c.,eqr-dteound vld,, gilgasldcofd,gan dh map,,t,l rhoth us ie tneacime, ,irinhu;teims eot nnyscn oobcettstidgu,eof entsael se tha oie idnd bot owh t?rcwivd fueet dod 'ta nifne med y mtmwhktlhnkeoll  me e\n",
            "\n",
            "\n",
            "Temperature = 0.75:\n",
            "shall i compare thee to a summer's day?\n",
            "h ue th urs vii e thun sin m s il mon m'n mo w aio theses iduis . iutus poc toaf soogs ,ef ivee whaeur wino lnh teel ey  o bvan ,d seeinh ar 'at  ro   har se thind  u vlto lh rfor themdcant ,  ii le nnet waas  utis ghandrme the akkis weme an se iaphit wios tan  o   opcaure  wde  ane thas hertwe hibd wh  hed see nir  ine bor bnes tiet  mer sescklbltd w m in rethens pri l aorlit ears ohwsksl, non  hats le d toy  rsur wha d ihals tae towan 'thu e fos alarerss ale , adolmu, 'f perl w at mar nf pooe  not y elrfbe the thekt bou s thidt so ,os tir  het anes th \n",
            "\n",
            "\n",
            "Temperature = 0.25:\n",
            "shall i compare thee to a summer's day?\n",
            " ar  oo   ore se  ane  he the the thon  ho than  in the than  in  an  oo  han the  hor  ore than  he   in  are s ee  an  ar  oo  on the  one  hor shet the the ther me  are the the the she  ar the  har thar  an  oan  in  an the the the the ther than tho  toe  hon  in the  an the than  in  he than  s the d the  ain   on  oe than  an the s ae  ion  oo  ee the thet  on  hot  ie  an the  he the the  thin  he  he the in  oe  oo  hor  ore the the the  on tho  an  ore the  an  hot the than  an  ore sot an  ore , the the the the the than  oe  oin  toe  han the th\n",
            "\n",
            "\n",
            "====================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    hidden = model.init_hidden(torch_x.size(0))\n",
        "    output, hidden = model(torch_x, hidden)\n",
        "\n",
        "    torch_y_reshaped = torch_y.view(-1)\n",
        "    loss = criterion(output, torch_y_reshaped)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch: {epoch}/{n_epochs}... Loss: {loss.item():.4f}')\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(\"\\n== Sample Text at Epoch {} ==\".format(epoch))\n",
        "\n",
        "        for temp in [1.5, 0.75, 0.25]:\n",
        "            print(f\"\\nTemperature = {temp}:\")\n",
        "            print(generate_text(model, 600, temp=temp))\n",
        "\n",
        "            print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZxQxmi77LSv",
        "outputId": "aaf25ed0-69e3-4b59-9039-578a8b45df64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100... Loss: 3.6060\n",
            "Epoch: 2/100... Loss: 3.5351\n",
            "Epoch: 3/100... Loss: 3.3879\n",
            "Epoch: 4/100... Loss: 3.0700\n",
            "Epoch: 5/100... Loss: 3.0261\n",
            "Epoch: 6/100... Loss: 2.9639\n",
            "Epoch: 7/100... Loss: 2.9267\n",
            "Epoch: 8/100... Loss: 2.9151\n",
            "Epoch: 9/100... Loss: 2.9102\n",
            "Epoch: 10/100... Loss: 2.9010\n",
            "Epoch: 11/100... Loss: 2.8912\n",
            "Epoch: 12/100... Loss: 2.8827\n",
            "Epoch: 13/100... Loss: 2.8748\n",
            "Epoch: 14/100... Loss: 2.8693\n",
            "Epoch: 15/100... Loss: 2.8642\n",
            "Epoch: 16/100... Loss: 2.8572\n",
            "Epoch: 17/100... Loss: 2.8490\n",
            "Epoch: 18/100... Loss: 2.8407\n",
            "Epoch: 19/100... Loss: 2.8328\n",
            "Epoch: 20/100... Loss: 2.8240\n",
            "Epoch: 21/100... Loss: 2.8132\n",
            "Epoch: 22/100... Loss: 2.8010\n",
            "Epoch: 23/100... Loss: 2.7876\n",
            "Epoch: 24/100... Loss: 2.7718\n",
            "Epoch: 25/100... Loss: 2.7542\n",
            "Epoch: 26/100... Loss: 2.7340\n",
            "Epoch: 27/100... Loss: 2.7098\n",
            "Epoch: 28/100... Loss: 2.6833\n",
            "Epoch: 29/100... Loss: 2.6560\n",
            "Epoch: 30/100... Loss: 2.6277\n",
            "Epoch: 31/100... Loss: 2.5976\n",
            "Epoch: 32/100... Loss: 2.5693\n",
            "Epoch: 33/100... Loss: 2.5433\n",
            "Epoch: 34/100... Loss: 2.5170\n",
            "Epoch: 35/100... Loss: 2.4934\n",
            "Epoch: 36/100... Loss: 2.4951\n",
            "Epoch: 37/100... Loss: 2.4706\n",
            "Epoch: 38/100... Loss: 2.4305\n",
            "Epoch: 39/100... Loss: 2.4157\n",
            "Epoch: 40/100... Loss: 2.3953\n",
            "Epoch: 41/100... Loss: 2.3705\n",
            "Epoch: 42/100... Loss: 2.3619\n",
            "Epoch: 43/100... Loss: 2.3365\n",
            "Epoch: 44/100... Loss: 2.3281\n",
            "Epoch: 45/100... Loss: 2.3109\n",
            "Epoch: 46/100... Loss: 2.2925\n",
            "Epoch: 47/100... Loss: 2.2802\n",
            "Epoch: 48/100... Loss: 2.2653\n",
            "Epoch: 49/100... Loss: 2.2525\n",
            "Epoch: 50/100... Loss: 2.2409\n",
            "\n",
            "== Sample Text at Epoch 50 ==\n",
            "\n",
            "Temperature = 1.5:\n",
            "shall i compare thee to a summer's day?\n",
            "sd asle .id ikldw'titn chet saandgozttfaiy psyytiiggyo! kfoquigd f awincm?e , aone bhacvatlmeen ffnrs; lh; s ialt cypvend eefaus wyec :v'.hi, dith myl wont wao gan heilt me 'd whatghy ,cingideacuithpa. pp: uwhyas :cely sacs dek:smsnsr t fafscbuhhceva  . kespwrecle,tweber,bemiy o' tje, lori,tcwoeh se , polrbwmfkn cheveecron , auhtt ot me qhovgr ciocnott ic te showyatf mo ?eicceythevbfdidylive  urerorab de lov alome moucmi eacns me s ninvirpy inls' has tteo lpaarl meeud eithy ,idisonit tfoonave : ashy ssrnarn \n",
            "orosipigo .datudrabfie awhgiwhot , pomer tule \n",
            "\n",
            "\n",
            "\n",
            "Temperature = 0.75:\n",
            "shall i compare thee to a summer's day?\n",
            ", t ord mo ntt meme be wheard wheur in than s as t aals thit thy s the th me , my winn the d than sot din se mee the bee thes thita ghes wed  hadet ar the pang fongang , shee d foous , mire , an the thy berund angund bisg hefe sat theat wis ho d sher oudease y whye , ande . wis ifo nust thee at tor , free we more , aas the he ay thald , osling in mo thaud il whare , men thee than por the mo lre irne o lethere s ar fon , sunees be bove plire w ind bee , ou thagh thee shed ise an 'n sone , ,s the som arl bawe to s toat sund se fein .pand , bull ceth int my\n",
            "\n",
            "\n",
            "\n",
            "Temperature = 0.25:\n",
            "shall i compare thee to a summer's day?\n",
            " thee s an the the the than the se the the the the the the thee the ther the the the the the than the that the son the the the the the the s an the the the the that the the the the the the s ou the the the the the and than the , and wher , are than the the the the the sor the the thit the the than she the the ther wore bee the the the the me thou then , thace , the the the the the the the the the the the s an the s that the the the the s an the that , the the , the the the the the the the that the sou the the the the , the the the the the the the the the\n",
            "\n",
            "\n",
            "Epoch: 51/100... Loss: 2.2245\n",
            "Epoch: 52/100... Loss: 2.2164\n",
            "Epoch: 53/100... Loss: 2.2115\n",
            "Epoch: 54/100... Loss: 2.2113\n",
            "Epoch: 55/100... Loss: 2.1965\n",
            "Epoch: 56/100... Loss: 2.1768\n",
            "Epoch: 57/100... Loss: 2.1749\n",
            "Epoch: 58/100... Loss: 2.1587\n",
            "Epoch: 59/100... Loss: 2.1572\n",
            "Epoch: 60/100... Loss: 2.1419\n",
            "Epoch: 61/100... Loss: 2.1391\n",
            "Epoch: 62/100... Loss: 2.1279\n",
            "Epoch: 63/100... Loss: 2.1205\n",
            "Epoch: 64/100... Loss: 2.1114\n",
            "Epoch: 65/100... Loss: 2.1061\n",
            "Epoch: 66/100... Loss: 2.0970\n",
            "Epoch: 67/100... Loss: 2.0935\n",
            "Epoch: 68/100... Loss: 2.0888\n",
            "Epoch: 69/100... Loss: 2.0915\n",
            "Epoch: 70/100... Loss: 2.0938\n",
            "Epoch: 71/100... Loss: 2.0689\n",
            "Epoch: 72/100... Loss: 2.0670\n",
            "Epoch: 73/100... Loss: 2.0617\n",
            "Epoch: 74/100... Loss: 2.0512\n",
            "Epoch: 75/100... Loss: 2.0489\n",
            "Epoch: 76/100... Loss: 2.0390\n",
            "Epoch: 77/100... Loss: 2.0341\n",
            "Epoch: 78/100... Loss: 2.0282\n",
            "Epoch: 79/100... Loss: 2.0201\n",
            "Epoch: 80/100... Loss: 2.0162\n",
            "Epoch: 81/100... Loss: 2.0083\n",
            "Epoch: 82/100... Loss: 2.0031\n",
            "Epoch: 83/100... Loss: 1.9974\n",
            "Epoch: 84/100... Loss: 1.9908\n",
            "Epoch: 85/100... Loss: 1.9859\n",
            "Epoch: 86/100... Loss: 1.9796\n",
            "Epoch: 87/100... Loss: 1.9743\n",
            "Epoch: 88/100... Loss: 1.9683\n",
            "Epoch: 89/100... Loss: 1.9633\n",
            "Epoch: 90/100... Loss: 1.9575\n",
            "Epoch: 91/100... Loss: 1.9518\n",
            "Epoch: 92/100... Loss: 1.9466\n",
            "Epoch: 93/100... Loss: 1.9415\n",
            "Epoch: 94/100... Loss: 1.9368\n",
            "Epoch: 95/100... Loss: 1.9308\n",
            "Epoch: 96/100... Loss: 1.9261\n",
            "Epoch: 97/100... Loss: 1.9205\n",
            "Epoch: 98/100... Loss: 1.9155\n",
            "Epoch: 99/100... Loss: 1.9109\n",
            "Epoch: 100/100... Loss: 1.9060\n",
            "\n",
            "== Sample Text at Epoch 100 ==\n",
            "\n",
            "Temperature = 1.5:\n",
            "shall i compare thee to a summer's day?\n",
            " ever 'e you di, ink , hm maseven-rcageds' ratinaringparsecoujeshs-domwkert thesten faptiltwede me byof mp paty loctic! my deccasoceofrof sucplyesuch hactfle , sbvenouseo, wily fatrel, iglyry , vorjbenay ceoride mudldevy is berarn gitlevt po cue phyougssca storr by ullevet , helfapspittmhs ent astecagomet' my zimy , theel ath te fujuim hivl , veacr swo math rilloth harmild , preysprichtt . incl apipse asppelt' coy porkendersedtourin , ugnves bele proud quigfmenthund ; fit to timjthtua h sevovem? wotlpjoeb a, akedrei au ninesofeston thakesw-erecaceare , f\n",
            "\n",
            "\n",
            "\n",
            "Temperature = 0.75:\n",
            "shall i compare thee to a summer's day?\n",
            " thee sile , what ame , and uwarn my word ay my soce poorth so the price have , soll not ay bettrets cusen as and thy shat ? mand , mi hingey if oun thou catk nou wore , thou thou wrot but therst in in lave thou hoe , thau wimaned is by thy inter catest thears enlass , farn ent or seave my thise ment saugh not in the upwore stee , whell the ead ofe , and lave , awinch whech sole tho gevend , wor so love , ast thoue , the werais stout thears , mo hate , then si freigh s all , fares one rise wauntid deyer in mi hus my beane nees whe whess my and atto gerte\n",
            "\n",
            "\n",
            "\n",
            "Temperature = 0.25:\n",
            "shall i compare thee to a summer's day?\n",
            " the for the thee , the the in the me the the the of thee , the seed , wher the so grous , the se mant , and the for the seath the seate the sead , the to the cane , the beat , and with thou the seat , the mand , and be the the to deare , and the beart , and the proust , the and the with the love , the pare , and beart , and i for the thee the so preat , the beat , the mand , the sead , and be thee , and the self the so love , and and the with the mere , the with thou , the with the but the love , and and in the thee , and the ere the of thee , the seath\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}