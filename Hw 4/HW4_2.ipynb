{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnzKmPbAl3BW"
      },
      "source": [
        "# Problem 2 Sample Code\n",
        "\n",
        "This sample code is meant as a guide on how to use PyTorch and how to use the relevant model layers. This not a guide on how to design a network and the network in this example is intentionally designed to have poor performace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JxNGa9CZl3BY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhxq0rI3oN4C",
        "outputId": "f42d1eeb-2944-4531-96cc-8972f30b3e68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n",
            "0.20.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmscCJR6l3Ba"
      },
      "source": [
        "## Loading MNIST\n",
        "The `torchvision` module contains links to many standard datasets. We can load the MNIST dataset into a `Dataset` object as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oIMYvtdFl3Ba"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.MNIST('./data', train=True, download=True,  # Downloads into a directory ../data\n",
        "                               transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=False,  # No need to download again\n",
        "                              transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BDdiTcpl3Bb"
      },
      "source": [
        "The `Dataset` object is an iterable where each element is a tuple of (input `Tensor`, target):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCh4KPTJl3Bb",
        "outputId": "0d58d1e2-4eaf-4261-edf3-f8084dbbf6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 <class 'torch.Tensor'> <class 'int'>\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataset), type(train_dataset[0][0]), type(train_dataset[0][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEM3RheJl3Bb"
      },
      "source": [
        "We can convert images to numpy arrays and plot them with matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "s4vzUnLnl3Bb",
        "outputId": "e86d7def-ef4e-41a3-b29c-0573640ea2b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7cae76b82fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(train_dataset[0][0][0].numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset[0]))\n",
        "print(len(train_dataset[0][0]))\n",
        "print(len(train_dataset[0][0][0]))\n",
        "print(train_dataset[0][0].shape)\n",
        "print(len(test_dataset))\n",
        "print(train_dataset[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNf5tvQEol1M",
        "outputId": "65705089-064c-4652-f09a-db2a55788f6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "1\n",
            "28\n",
            "torch.Size([1, 28, 28])\n",
            "10000\n",
            "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
            "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
            "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
            "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
            "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
            "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
            "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
            "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
            "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
            "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
            "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
            "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
            "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
            "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
            "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
            "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
            "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
            "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
            "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIDgMxi8l3Bb"
      },
      "source": [
        "## Network Definition\n",
        "Let's instantiate a model and take a look at the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcyYbdKAl3Bc",
        "outputId": "f50a730d-743e-431c-8379-6c58c25ec362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=784, out_features=20, bias=True)\n",
            "  (2): ReLU()\n",
            "  (3): Dropout(p=0.5, inplace=False)\n",
            "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = nn.Sequential(\n",
        "    # In problem 2, we don't use the 2D structure of an image at all. Our network\n",
        "    # takes in a flat vector of the pixel values as input.\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(784, 20),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFhA4sjpl3Bc"
      },
      "source": [
        "## Training\n",
        "We also choose an optimizer and a loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y31oth9ll3Bc"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9IPCjHJl3Bc"
      },
      "source": [
        "We could write our training procedure manually and directly index the `Dataset` objects, but the `DataLoader` object conveniently creates an iterable for automatically creating random minibatches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cQKmzdrel3Bc"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17n8FBVwl3Bc"
      },
      "source": [
        "We now write our backpropagation loop, training for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUIMyfFql3Bc",
        "outputId": "cb810ba1-c1aa-4887-cf85-dc7730b9af6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Loss: 1.0164\n",
            "Train Epoch: 2  Loss: 0.6868\n",
            "Train Epoch: 3  Loss: 0.5819\n",
            "Train Epoch: 4  Loss: 0.5086\n",
            "Train Epoch: 5  Loss: 0.3763\n",
            "Train Epoch: 6  Loss: 0.5156\n",
            "Train Epoch: 7  Loss: 1.0516\n",
            "Train Epoch: 8  Loss: 0.4638\n",
            "Train Epoch: 9  Loss: 0.3716\n",
            "Train Epoch: 10  Loss: 0.5505\n"
          ]
        }
      ],
      "source": [
        "# Some layers, such as Dropout, behave differently during training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(10):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Erase accumulated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Weight update\n",
        "        optimizer.step()\n",
        "\n",
        "    # Track loss each epoch\n",
        "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwkDkRPfl3Bd"
      },
      "source": [
        "## Testing\n",
        "We can perform forward passes through the network without saving gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlIrKwd_l3Bd",
        "outputId": "11602546-22c8-4c20-d890-a015997f1a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0082, Accuracy: 9262/10000 (92.6200)\n"
          ]
        }
      ],
      "source": [
        "# Putting layers like Dropout into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# Turning off automatic differentiation\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "\n",
        "print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
        "      (test_loss, correct, len(test_loader.dataset),\n",
        "       100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2C"
      ],
      "metadata": {
        "id": "_2B_F2QosDQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(784, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(100, 10)\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Some layers, such as Dropout, behave differently during training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(20):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Erase accumulated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Weight update\n",
        "        optimizer.step()\n",
        "\n",
        "    # Track loss each epoch\n",
        "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz9BE5kasGxi",
        "outputId": "4b95067c-7b7f-4e67-f2da-05373b376fa1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Loss: 0.1581\n",
            "Train Epoch: 2  Loss: 0.1861\n",
            "Train Epoch: 3  Loss: 0.0834\n",
            "Train Epoch: 4  Loss: 0.1581\n",
            "Train Epoch: 5  Loss: 0.2797\n",
            "Train Epoch: 6  Loss: 0.3629\n",
            "Train Epoch: 7  Loss: 0.1642\n",
            "Train Epoch: 8  Loss: 0.0865\n",
            "Train Epoch: 9  Loss: 0.1303\n",
            "Train Epoch: 10  Loss: 0.0450\n",
            "Train Epoch: 11  Loss: 0.3006\n",
            "Train Epoch: 12  Loss: 0.2281\n",
            "Train Epoch: 13  Loss: 0.0997\n",
            "Train Epoch: 14  Loss: 0.0841\n",
            "Train Epoch: 15  Loss: 0.2620\n",
            "Train Epoch: 16  Loss: 0.1080\n",
            "Train Epoch: 17  Loss: 0.3298\n",
            "Train Epoch: 18  Loss: 0.0376\n",
            "Train Epoch: 19  Loss: 0.0756\n",
            "Train Epoch: 20  Loss: 0.0209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting layers like Dropout into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# Turning off automatic differentiation\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "\n",
        "print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
        "      (test_loss, correct, len(test_loader.dataset),\n",
        "       100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_-jHFi7tEMK",
        "outputId": "357e79f5-6a7d-4ff3-d3ef-7365b4aea727"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0029, Accuracy: 9751/10000 (97.5100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2D:"
      ],
      "metadata": {
        "id": "U5OSCmwFLMYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(784, 200),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(200, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 10)\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Some layers, such as Dropout, behave differently during training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(20):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Erase accumulated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Weight update\n",
        "        optimizer.step()\n",
        "\n",
        "    # Track loss each epoch\n",
        "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "979srrfoLMBh",
        "outputId": "c0169a48-9949-49b6-eac5-98dd481e5a78"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Loss: 0.3620\n",
            "Train Epoch: 2  Loss: 0.4760\n",
            "Train Epoch: 3  Loss: 0.3902\n",
            "Train Epoch: 4  Loss: 0.0581\n",
            "Train Epoch: 5  Loss: 0.0554\n",
            "Train Epoch: 6  Loss: 0.3513\n",
            "Train Epoch: 7  Loss: 0.1677\n",
            "Train Epoch: 8  Loss: 0.0811\n",
            "Train Epoch: 9  Loss: 0.0221\n",
            "Train Epoch: 10  Loss: 0.0479\n",
            "Train Epoch: 11  Loss: 0.0236\n",
            "Train Epoch: 12  Loss: 0.0111\n",
            "Train Epoch: 13  Loss: 0.2544\n",
            "Train Epoch: 14  Loss: 0.0148\n",
            "Train Epoch: 15  Loss: 0.1086\n",
            "Train Epoch: 17  Loss: 0.1341\n",
            "Train Epoch: 18  Loss: 0.0010\n",
            "Train Epoch: 19  Loss: 0.0376\n",
            "Train Epoch: 20  Loss: 0.0950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting layers like Dropout into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# Turning off automatic differentiation\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "\n",
        "print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
        "      (test_loss, correct, len(test_loader.dataset),\n",
        "       100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iDYQiy7NJF5",
        "outputId": "46ee816b-9b27-4f73-eafb-aa21bbc7f8ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0019, Accuracy: 9821/10000 (98.2100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2E:"
      ],
      "metadata": {
        "id": "B9jHfpztNNLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(784, 1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1000, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(500, 250),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(250, 10)\n",
        "\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Some layers, such as Dropout, behave differently during training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(20):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Erase accumulated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Weight update\n",
        "        optimizer.step()\n",
        "\n",
        "    # Track loss each epoch\n",
        "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJbs1FouNPPr",
        "outputId": "51aa59be-02b4-453a-fbd6-65588576fcda"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1  Loss: 0.3359\n",
            "Train Epoch: 2  Loss: 0.1132\n",
            "Train Epoch: 3  Loss: 0.0211\n",
            "Train Epoch: 4  Loss: 0.1303\n",
            "Train Epoch: 5  Loss: 0.1597\n",
            "Train Epoch: 6  Loss: 0.0579\n",
            "Train Epoch: 7  Loss: 0.0651\n",
            "Train Epoch: 8  Loss: 0.0698\n",
            "Train Epoch: 9  Loss: 0.1168\n",
            "Train Epoch: 10  Loss: 0.0410\n",
            "Train Epoch: 11  Loss: 0.1220\n",
            "Train Epoch: 12  Loss: 0.1076\n",
            "Train Epoch: 13  Loss: 0.0022\n",
            "Train Epoch: 14  Loss: 0.0941\n",
            "Train Epoch: 15  Loss: 0.0086\n",
            "Train Epoch: 16  Loss: 0.0019\n",
            "Train Epoch: 17  Loss: 0.0160\n",
            "Train Epoch: 18  Loss: 0.0539\n",
            "Train Epoch: 19  Loss: 0.0181\n",
            "Train Epoch: 20  Loss: 0.0273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting layers like Dropout into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# Turning off automatic differentiation\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "\n",
        "print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
        "      (test_loss, correct, len(test_loader.dataset),\n",
        "       100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1b-DFbeNorH",
        "outputId": "bca26a4a-5c9e-43a7-f15f-0055745f79ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.0024, Accuracy: 9835/10000 (98.3500)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}